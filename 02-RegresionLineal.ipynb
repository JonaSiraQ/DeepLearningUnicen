{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal\n",
    "La regresión lineal es la aplicación de un modelo lineal entre una variable dependiente ($y$) y una o más variables dependientes ($X$).\n",
    "\n",
    "$$\\hat{y}=x_0w_0+x_1w_1+...+x_iw_i+b$$\n",
    "\n",
    "donde, considerando un error $\\varepsilon$:\n",
    "\n",
    "$$y = \\hat{y}+\\varepsilon$$\n",
    "\n",
    "Siendo el caso de una variable:\n",
    "\n",
    "$$\\hat{y}=xw+b$$\n",
    "\n",
    "A continuación se presenta un ejemplo basado en datos generados. Para este ejemplo se generarán 50 puntos con la siguiente distribución:\n",
    "\n",
    "$$y=3*x+(rand-0.5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gen_random_data(mult):\n",
    "    _x = np.linspace(-1, 1, 50)\n",
    "    _error = (np.random.rand(*_x.shape) - .5)\n",
    "    _y = _x * mult + _error\n",
    "    return _x, _y\n",
    "\n",
    "\n",
    "x, y = gen_random_data(3)\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.show()\n",
    "print('x: {}'.format(x))\n",
    "print('y: {}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "Considerando la varaible independiente $x$ y la variable dependiente $y$, el objetivo de un regresión lineal es encontrar $w$ y $b$ tal que dada una función de error $E(y, \\hat{y})$ sea mínimo. Es decir:\n",
    "\n",
    "$$\\underset{w,b}{arg\\,min}=E(y,xw+b)$$\n",
    "\n",
    "### Ejercicio\n",
    "Complete la definición de la función `lineal` presentada a continuación, y grafique la función sobre los datos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineal(x, w, b):\n",
    "    return x# su respuesta aquí\n",
    "\n",
    "\n",
    "plt.plot(x, y, 'ro', x, lineal(x, 3, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de error\n",
    "Una función de error utilizada para este tipo de problemas es el error medio cadrático (_mean squared error_), que se define como:\n",
    "\n",
    "$$MSE(y,\\hat{y})=\\frac{1}{N}\\sum(y-\\hat{y})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return 0 #Implemente le función de error\n",
    "\n",
    "print('El MSE es {}'.format(mse(y, lineal(x, 3, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización\n",
    "El problema en la regresión lineal es encontrar los parámetro que minimiza el valor de la función de error. A continuación se presenta un gráfico mostrando el valor de la función de $mse$ para diverso valores de $w$ y $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Construyendo datos\n",
    "w = np.arange(1, 5, 0.1)\n",
    "b = np.arange(-1, 1, 0.01)\n",
    "w, b = np.meshgrid(w, b)\n",
    "e = np.empty_like(w)\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(w.shape[1]):\n",
    "        e[i, j] = mse(y, lineal(x, w[i, j], b[i, j]))\n",
    "\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(w, b, e, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente, calculando el error para diversos valores de $w$ y $b$ se puede seleccionar el mínimo. Sin embargo, esto es impracticable cuando existen muchos parámetros o puntos de datos.\n",
    "Por simplicadas, a continuación vamos a suponer que se conoce $b=0$ resultando en que $\\hat{y}=xw$, por simplicidad la llamaremos $h(x)$. Entonces, nuestro único problema sería encontrar $w$. En este caso, si graficamos la curva de error obtendríamos lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_error(y, x, ws):\n",
    "    def single_error(w):\n",
    "        return mse(y, lineal(x, w, 0))\n",
    "    _s = np.vectorize(single_error)\n",
    "    return _s(ws)\n",
    "\n",
    "ws = np.linspace(1, 5, 51)\n",
    "plt.plot(ws, exp_error(y, x, ws))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución\n",
    "Dado que la función de error tiene un solo mínimo, se podrían tomar 2 valores cercanos de manera de conocer en qué dirección es conveniente explorar. La función lineal en realidad es una función que depende no solo de los datos $x$, sino que también del parámetro a aprender $w$, entonces la notaremos como $h(x,w)$. Para conocer la pendiente de la función de error dado el parámetro a conocer debemos hacer:\n",
    "\n",
    "$$pendiente_w(w_{1}, w_{0})=\\frac{MSE(y,h(x,w_{1}))-MSE(y,h(x,w_{0}))}{w_{1}-w_{0}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = exp_error(y, x, ws)\n",
    "pendiente = (errors[10]-errors[20])/(ws[10]-ws[20])\n",
    "correccion_ordenada_origen = -pendiente*ws[10] + errors[10]\n",
    "plt.plot(ws, errors, ws[10:21], lineal(pendiente, ws[10:21], 0)+correccion_ordenada_origen)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, se podría inicializar $w$ de forma aleatoria e ir actualizando el valor en contra de la pendiende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pendiente(y_true, x, w, delta=1e-6):\n",
    "    return (mse(y_true, lineal(x, w + delta, 0))-mse(y_true, lineal(x, w, 0))) / delta\n",
    "\n",
    "w = 0 #Podría ser cualquier valor\n",
    "ciclos = 100\n",
    "lr = 0.1 \n",
    "errors = []\n",
    "for i in range(ciclos):\n",
    "    p = pendiente(y, x, w)\n",
    "    errors.append(mse(y, lineal(x, w, 0)))\n",
    "    w = w - lr * p\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "Este método se puede usar para aproximar todos los parámetros de la función lineal, es decir, tanto $w$ y $b$.\n",
    "\n",
    "Con estas pendientes se actualizarán iterativamente los valores siguiendo el siguiente esquema:\n",
    "\n",
    "```for i in range(ciclos):\n",
    "    pw, pb = pendiente(w, b)\n",
    "    w = w - lr * pw\n",
    "    b = b - lr * pb\n",
    "```\n",
    "\n",
    "Donde `lr` (_learning rate_ o taza de aprendizaje) es un hiperparámetro (parámetro que no se aprende, sino que es definido por el desarrollador) que indica cuan agresivamente se aprende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pendiente(y_true, x, w, b, delta=1e-6):\n",
    "    pw = 0#compeltar\n",
    "    pb = 0#compeltar\n",
    "    return pw, pb\n",
    "\n",
    "w = 5 #Podría ser cualquier valor\n",
    "b = 5\n",
    "ciclos = 100\n",
    "lr = 0.1 \n",
    "errors = []\n",
    "for i in range(ciclos):\n",
    "    pw, pb = pendiente(y, x, w, b)\n",
    "    errors.append(mse(y, lineal(x, w, b)))\n",
    "    w = w - lr * pw\n",
    "    #Actualicé el valor de b\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "### Pendiente de $w$\n",
    "Desde el punto de vista de la pendiente de $w$, si definimos a $w_{1}=w_{0}+\\Delta$ entonces:\n",
    "\n",
    "$$\\lim_{\\Delta \\rightarrow 0} pendiente_w(w_{0}+\\Delta,w_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w_{0}+\\Delta, b))-MSE(y,h(x,w_{0}, b))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{dw}$$\n",
    "\n",
    "Esta derivada se puede resolver por regla de la cadena:\n",
    "\n",
    "$$\\frac{dMSE(y,h(x,w,b))}{dw}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{dw}$$\n",
    "\n",
    "La primer derivada se resuelve, devuelta por regla de la cadena:\n",
    "\n",
    "$$\\frac{dMSE(y,h(x,w,b))}{d(h(x,w,b)}=\\frac{d(\\frac{1}{N}\\sum(y-h(x,w,b))^{2}}{d(h(x,w,b))}=-\\frac{2}{N}\\sum(y-h(x,w,b))$$\n",
    "\n",
    "La segunda derivada se resuelve así:\n",
    "\n",
    "$$\\frac{dh(x,w,b)}{dw}=\\frac{d(xw+b)}{dw}=x$$\n",
    "\n",
    "Finalmente, resulta en:\n",
    "\n",
    "$$\\frac{dMSE(y,h(x,w,b))}{dw}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{dw}=\\frac{-2\\sum(y-(xw+b))*x}{N}$$\n",
    "\n",
    "### Pendiente $b$\n",
    "Desde el punto de vista de la pendiente de $b$, si definimos a $b_{1}=b_{0}+\\Delta$ entonces:\n",
    "\n",
    "$$\\lim_{\\Delta \\rightarrow 0} pendiente_b(b_{0}+\\Delta,b_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w, b_{0}+\\Delta))-MSE(y,h(x,w,b_{0}))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{db}$$\n",
    "\n",
    "Esta derivada se puede resolver por regla de la cadena:\n",
    "\n",
    "$$\\frac{dMSE(y,h(x,w,b))}{db}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{db}$$\n",
    "\n",
    "La primer derivada ya se resolvió arriba.\n",
    "\n",
    "La segunda derivada se resuelve así:\n",
    "\n",
    "$$\\frac{dh(x,w,b)}{db}=\\frac{d(xw+b)}{db}=1$$\n",
    "\n",
    "Finalmente, resulta en:\n",
    "\n",
    "$$\\frac{dMSE(y,h(x,w,b))}{db}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{db}=\\frac{-2\\sum(y-(xw+b))*1}{N}$$\n",
    "### Ejercicio\n",
    "Implemente la función gradiente considerando las derivadas de $\\frac{dMSE(y,h(x,w,b))}{dw}$ y $\\frac{dMSE(y,h(x,w,b))}{db}$, ejecute la función `gradient_check()` para verificar su implementación:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "def gradiente(y_true, x, w, b):\n",
    "    gw = 0#Implemente aqui el gradiente w\n",
    "    gb = 0#Implemente aqui el gradiente b\n",
    "    return gw, gb\n",
    "               \n",
    "def gradient_check(x, y_true, ciclos=100, delta=1e-6, error=1e-5):\n",
    "    ok = True\n",
    "    for i in range(ciclos):\n",
    "        w = uniform(-50, 50) #genera un flotante aleatorio\n",
    "        b = uniform(-50, 50) #genera un flotante aleatorio\n",
    "        pw, pb = pendiente(y_true, x, w, b, delta)\n",
    "        gw, gb = gradiente(y_true, x, w, b)\n",
    "        if abs(pw - gw) > error or abs(pb - gb) > error:\n",
    "            print('Error para w={}, b={}. pw={}, pb={}, gw={},gb={}'.format(w, b, pw, pb, gw, gb))\n",
    "            print(pw-gw)\n",
    "            print(pb-gb)\n",
    "            ok = False\n",
    "    if ok:\n",
    "        print('No hubo errores en la prueba')\n",
    "        \n",
    "gradient_check(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "Implemente el algoritmo para aprender parámetros que se utilizó anteriormente, pero en vez de utilizar las pendientes, utilize el gradiente. Grafique el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = uniform(-50, 50) #genera un flotante aleatorio\n",
    "b = uniform(-50, 50) #genera un flotante aleatorio\n",
    "print('Valores iniciales. w={} b={}'.format(w, b))\n",
    "ciclos = 100\n",
    "lr = 0.1\n",
    "#Incluya su código para entrenar la regresión lineal\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizando para problemas con multiples variables\n",
    "Cuando el problema en el que se quiere encontrar una relación entre multiples variables, es decir $\\bar{X}=(x_0, x_1, ..., x_n)$, y un valor objetivo $y$, los valores a optimizar son $\\bar{W}=(w_0, w_1, ..., w_n)$ y $b$.\n",
    "\n",
    "$$\\hat{y}=\\bar{X}\\cdot\\bar{W}+b=x_0w_0+x_1w_1+...+x_iw_i+b$$\n",
    "\n",
    "Si se considera que se puede tener muchas instancias, $\\bar{X_0}, \\bar{X_1},..., \\bar{X_i}$, el problema de predecir todos los valores se puede expresar como un problema de matrices donde:\n",
    "\n",
    "$$X= \\left[\\begin{array}{c}\n",
    "\\bar{X_{0}}^{T}\\\\\n",
    "\\bar{X_{1}}^{T}\\\\\n",
    "\\vdots\\\\\n",
    "\\bar{X_{n}}^{T}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "$$\\hat{Y}=X\\cdot\\bar{W}+b$$\n",
    "\n",
    "Es importante notar que:\n",
    "$$\\frac{dMSE(\\bar{Y},h(X,\\bar{W},b))}{dw_n}=\\frac{dMSE(\\bar{Y},h(X,\\bar{W},b))}{d(h(X,\\bar{W},b))}.\\frac{(h(X,\\bar{W},b))}{dw_n}$$\n",
    "Donde la primera derivada se resuelve como\n",
    "$$\\frac{dMSE(\\bar{Y},h(X,\\bar{W},b))}{d(w_n))}=\\frac{d(\\frac{1}{N}\\sum(\\bar{Y}-h(X,\\bar{W},b))^{2}}{d(h(X,\\bar{W},b))}=-\\frac{2}{N}\\sum((\\bar{Y}-h(X,\\bar{W},b))\\bar{x_n})$$\n",
    "\n",
    "La porque segunda derivada se resuelve así:\n",
    "\n",
    "$$\\frac{dh(X,\\bar{W},b)}{dw_n}=\\frac{d(x_0w_0+x_1w_1+...+x_iw_i+b)}{dw_n}=\\bar{x_n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lineal(x, w, b):\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "def gradiente(y_true, x, w, b):\n",
    "    dm = -2 * (y_true - lineal(x, w, b))\n",
    "    gw = np.transpose(np.average(np.transpose(x) * dm, axis=1))\n",
    "    gb = np.average(dm)\n",
    "    return gw, gb\n",
    "\n",
    "w = np.asarray([1,2,3])\n",
    "x = np.asarray([[1,2,3], [2,3,4]])\n",
    "b = 1\n",
    "y = np.asarray([10, 20])\n",
    "print(lineal(x, w, b))\n",
    "print(gradiente(y, x, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "En el siguiente segmento se inicializa datos tomados de los dataset de ejemplo provistos por [Weka](https://www.cs.waikato.ac.nz/~ml/weka/datasets.html). En partícular, el dataset `Housing.arff`[1] que se encuentra en el archivo `datasets-numeric.jar`. El dataset contiene 506 puntos de datos con 14 atributos (13 se asumen independientes y uno dependiente). Los atributos son de acuerdo a lo documentado en el archivo `arff`:\n",
    "1. __CRIM__      per capita crime rate by town.\n",
    "2. __ZN__        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. __INDUS__     proportion of non-retail business acres per town.\n",
    "4. __CHAS__      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "5. __NOX__       nitric oxides concentration (parts per 10 million).\n",
    "6. __RM__        average number of rooms per dwelling.\n",
    "7. __AGE__       proportion of owner-occupied units built prior to 1940.\n",
    "8. __DIS__       weighted distances to five Boston employment centres.\n",
    "9. __RAD__       index of accessibility to radial highways.\n",
    "10. __TAX__      full-value property-tax rate per \\$10,000.\n",
    "11. __PTRATIO__  pupil-teacher ratio by town.\n",
    "12. __B__        1000(Bk - 0.63)^2 where Bk is the proportion of blacks.\n",
    "13. __LSTAT__    % lower status of the population.\n",
    "14. __MEDV__     Median value of owner-occupied homes in \\$1000's. <- Valor a predecir.\n",
    "\n",
    "[1] Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "\n",
    "### Explicación del código\n",
    "Los datos ya se encuentran almacenados en una matrix de Numpy, que está en el archivo `02-housing.npy`. Esta matriz es de 506 por 14 elementos. La última columna tiene los valores de __MEDV__, es decir nuestro objetivos de la regresión. Debido a que se desconoce si los datos originales están almacenados con algún orden, se reordenan las instancias de forma aleatoria. De las 506 instancias 400 son tomadas para el conjunto de entrenamiento y 106 para el de evaluación. Finalmente, los atributos independientes son escalados a valores entre [0, 1] tomando como base los mínimos y máximos de los atributos de entrenamiento.\n",
    "\n",
    "__Nota__: Probar que pasa si no se hace este escalado. Imprimir los gradientes, cambiar el `lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "while not os.path.exists('02-housing.npy'):\n",
    "    #Si no está el archivo hay que subirlo. Solo para Google Colab!!\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
    "data = np.load('02-housing.npy')\n",
    "data = data.astype(np.float32)\n",
    "np.random.shuffle(data)\n",
    "x_train = data[:400,:13]\n",
    "y_train = data[:400,13].reshape((400,)) #Transforma de una matrix de (400, 1) a un vector de (400,)\n",
    "x_test = data[400:,:13]\n",
    "y_test = data[400:,13].reshape((106,)) #Transforma de una matrix de (400, 1) a un vector de (400,)\n",
    "maxs = np.max(x_train, axis=0)\n",
    "mins = np.min(x_train, axis=0)\n",
    "x_train = (x_train - mins) / (maxs - mins)\n",
    "x_test = (x_test - mins) / (maxs - mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(13)\n",
    "b = uniform(-10, 10)\n",
    "print('Valores iniciales. w={} b={}'.format(w, b))\n",
    "ciclos = 1000\n",
    "lr = 0.1\n",
    "errors = []\n",
    "errors_test = []\n",
    "for i in range(ciclos):\n",
    "    pw, pb = gradiente(y_train, x_train, w, b)\n",
    "    errors.append(mse(y_train, lineal(x_train, w, b)))\n",
    "    errors_test.append(mse(y_test, lineal(x_test, w, b)))\n",
    "    w = w - lr * pw\n",
    "    b = b - lr * pb \n",
    "print('Errores a medida que se actualiza el valor de w y b')\n",
    "plt.plot(range(ciclos), errors, 'b-', range(ciclos), errors_test, 'r-')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lineal(x_test, w, b)\n",
    "print('El error para el conjunto de prueba es {}'.format(mse(y_test, y_pred)))\n",
    "from scipy.stats import pearsonr\n",
    "print('La correalción de Pearson entre los valores reales y los predichos es {}, con un pvalue: {}'.format(*pearsonr(y_test, y_pred)))\n",
    "plt.plot(y_test, y_pred, 'r*')\n",
    "plt.xlabel('Valores reales')\n",
    "plt.ylabel('Valores predichos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "[Tensorflow](https://www.tensorflow.org/) en una librería de para aplicaciones de operaciones númericas de alta performace creada por Google con foco en inteligencia artificial y Deep Learning. Tensorflow soporta procesamiento en CPU, GPU y TPUs. Tensorflow provee ya implementada algunas operaciones comunes como calcular los gradientes de las variables o técnicas de entrenamiento comunes como _Gradient Descent_.\n",
    "A continuación, se muestra un ejemplo de como utilizar Tensorflow para realizar una regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "rng = np.random\n",
    "\n",
    "# Placeholder de las entradas\n",
    "X = tf.placeholder(tf.float32, [None, 13])\n",
    "Y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "W = tf.Variable(rng.randn(13).astype(np.float32), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# Modelo lineal\n",
    "lineal = tf.add(tf.reduce_sum(tf.matmul(X, tf.expand_dims(W, axis=1)), axis=1), b)\n",
    "\n",
    "# MSE\n",
    "cost = tf.reduce_mean(tf.pow(lineal-Y, 2))\n",
    "# Gradient descent\n",
    "# minimize() sabe que hay que modificar W y b porque están configuradas como trainable=True por defecto\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Comenzar una sessión\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Inicializar\n",
    "    sess.run(init)\n",
    "    y_pred = sess.run(lineal, feed_dict={X: x_test, Y:y_test})\n",
    "    print('El error para el conjunto de prueba es {}'.format(mse(y_test, y_pred)))\n",
    "    plt.plot(y_test, y_pred, 'r*')\n",
    "    plt.xlabel('Valores reales')\n",
    "    plt.ylabel('Valores predichos')\n",
    "    plt.show()\n",
    "    errors = []\n",
    "    errors_test = []\n",
    "    print('Entrenando')\n",
    "    for epoch in range(ciclos):\n",
    "        sess.run(optimizer, feed_dict={X: x_train, Y: y_train})\n",
    "        errors.append(sess.run(cost, feed_dict={X: x_train, Y:y_train}))\n",
    "        errors_test.append(sess.run(cost, feed_dict={X: x_test, Y:y_test}))\n",
    "    print('Error en entrenamiento')\n",
    "    plt.plot(range(ciclos), errors, 'b-', range(ciclos), errors_test, 'r-')\n",
    "    plt.show()\n",
    "    y_pred = sess.run(lineal, feed_dict={X: x_test, Y:y_test})\n",
    "    print('El error para el conjunto de prueba es {}'.format(mse(y_test, y_pred)))\n",
    "    print('La correalción de Pearson entre los valores reales y los predichos es {}, con un pvalue: {}'.format(*pearsonr(y_test, y_pred)))\n",
    "    plt.plot(y_test, y_pred, 'r*')\n",
    "    plt.xlabel('Valores reales')\n",
    "    plt.ylabel('Valores predichos')\n",
    "    plt.show()\n",
    "    print('El w final es {}'.format(sess.run(W)))\n",
    "    print('El b final es {}'.format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
