{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWmoTcyOSu5J"
   },
   "source": [
    "# Regresión logística\n",
    "La regresión logística (_Logistic Regression_) es un típo de regresión cuyo objetivo es determinar la probabilidad de que una instancia pertezca a una clase $y$, dado un conjunto de variables independientes $x_i$ que la definen. En este contexto, las instancias están representadas como un vector de variables independientes $x=\\mathbb{R}^{n}$ y una clase $y=\\{0,1\\}$. Es decir:\n",
    "\n",
    "$$P(y|x)=h(x)$$\n",
    "\n",
    "En este contexto, la función seleccionada para hacer esta estimación por excelencia es la sigmoide.\n",
    "\n",
    "$$sigmoid(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "## Ejercicio\n",
    "Grafíque la función sigmoid con $z$ en el rango $[-6, 6]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6de2_jIfSu5S"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#Ingrese su código aquí\n",
    "def sigmoid(x):\n",
    "    return 0\n",
    "\n",
    "x = np.linspace(-6, 6, 250)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAAKZBoiSu5k"
   },
   "source": [
    "Además de que la imagen de esta función está en $(0, 1)$, la derivada de esta función es:\n",
    "$$\\frac{sigmoid(z)}{dz}=sigmoid(z)(1-sigmoid(z))$$\n",
    "Lo que facilitaba su implementación.\n",
    "\n",
    "En este contexto, $z$ es una convinación lineal de las variables $x$.\n",
    "\n",
    "\n",
    "## Función de error\n",
    "Para calcular el error, se utiliza la entropía cruzada entre el valor esperado y el valor obtenido.\n",
    "$$CE(y,\\hat{y})=\\frac{\\sum(-y*log(\\hat{y})-(1-y)*log(1-\\hat{y}))}{N}$$\n",
    "En este contexto, la entropía curzada se interpreta como la información promedio (en bits) necesaria para determinar el valor de $y$ dado que se conoce el valor de $\\hat{y}$.\n",
    "\n",
    "__Nota__: Por simplicidad, se interpreta el logaritmo como logaritmo natural, pero se puede usar logaritmo en cualquier base, ya que solo afecta en una constante.\n",
    "\n",
    "__Ejercicio Opcional__: Calcule  $\\frac{d(CE(y,\\hat{y}))}{d\\hat{y}}$, $\\frac{d(CE(y,\\hat{y}))}{dw}$ $\\frac{d(CE(y,\\hat{y}))}{db}$. Asuma que $w$ es un escalar y considere que $y$ es $0$ o $1$. \n",
    "\n",
    "## Ejemplo\n",
    "Para el ejemplo de regresión logística se utilizará el conjunto de datos de cancer de pecho provisto. Este conjuntos de datos fue recolectado por investigadores de la Universisda de Wisconsin y provisto por la [UCI](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). Para acceder al conjunto de datos, no es necesario descargarlo y convertirlo al formato, ya que en encuentra provisto por el módulo de [_sklearn.datasets_](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) de la librería sickit-learn, que es una librería de _machine learning_ que se utilizará durante el curso.\n",
    "\n",
    "El dataset tiene 569 instancias, con 30 atributos cada una. Las instancias pueden ser clasificadas entre Malignos y Benignos. El dataset está ligeramente desbalanceado, lo que significa que existen más instancias de una clase que de la otra. En partícular, 37,25% de las instancias son Malignas y 62,75% son Benignas. La siguiente tabla resume el conjunto de datos:\n",
    "\n",
    "| Propiedad | Valor |\n",
    "| --- | --- |\n",
    "| Clases | 2 |\n",
    "| Ejemplos por clase | 212(M-0), 357(B-1) | \n",
    "| Total de instancias | 569 |\n",
    "| Dimensionalidad | 30|\n",
    "\n",
    "El siguiente código:\n",
    "1. Levanta los datos divididos en `x` (atributos) e `y` (clase).\n",
    "1. Divide los datos en entrenamiento y testing.\n",
    "1. Escala los datos de entrenamiento a valores entre 0 y 1.\n",
    "1. Aplica las correcciones de escalado al conjunto de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rYVLH0jSu5n"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "x, y = load_breast_cancer(True)\n",
    "x_train = x[:500,:]\n",
    "y_train = y[:500]\n",
    "x_test = x[500:,:]\n",
    "y_test = y[500:]\n",
    "\n",
    "maxs = np.max(x_train, axis=0)\n",
    "mins = np.min(x_train, axis=0)\n",
    "x_train = (x_train - mins) / (maxs - mins)\n",
    "x_test = (x_test - mins) / (maxs - mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqfDEve6Su51"
   },
   "source": [
    "### Visualizando el problema\n",
    "Muchas veces la visualización del problema ayuda a entender sus carácteristicas. Sin embargo, visualizar elementos en 30 dimensiones no es posible. Para esto existen diferentes técnicas que nos permiten reducir la dimensionalidad de los elementos manteniendo ciertas propiedades. Para visualizar los datos utilizaremos T-distributed Stochastic Neighbor Embedding (t-SNE) [1], que reduce la dimensiones manteniendo la distribución estadistica de las distancias entre los puntos de datos. \n",
    "\n",
    "El siguiente código aplica t-SNE a los datos de entrenamiento y los grafica utilizando estrellas azules para los benignos y rojas para los malignos. Es importante notar que los puntos obtenidos varian de ejecución en ejecución ya que t-SNE es sensible a su inicialización.\n",
    "\n",
    "[1] van der Maaten, L.J.P.; Hinton, G.E. (Nov 2008). [\"Visualizing High-Dimensional Data Using t-SNE\"](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). Journal of Machine Learning Research. 9: 2579–2605."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9T5ek2GSu55"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "ts_rep = TSNE().fit_transform(x_train)\n",
    "for point, label in zip(ts_rep, y_train):\n",
    "    rep = 'b*' if label == 1 else 'r*'\n",
    "    plt.plot([point[0]], [point[1]], rep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vThhA4RSu6I"
   },
   "source": [
    "Una vez visualizados los datos, hacemos una regresión logística sobre los datos. Para analizar el funcionamiento de la regresíon logística utilizaremos una matrix de confusión. En esta matriz las filas representan las clases reales y las columnas las clases predichas. En cada celda se encuentran la cantidad de instancias con la clase de la fila y la predicción asociada de la columna.\n",
    "\n",
    "| | Predición: 0  | Predición: 1 |\n",
    "| --- | --- | --- |\n",
    "| __Real: 0__ | Verdaderos Negativos | Falsos Positivos |\n",
    "| __Real: 1__ | Falsos Negativos | Verdaderos Positivos |\n",
    "\n",
    "Esta matriz permite visualizar fácilmente los valores de:\n",
    "* __Verdaderos Negativos__: cantidad de instancias clasificadas como negativas que efectivamente eran negativas. En nuestro ejemplo Maligno es la clase negativa porque tiene asociada el cero. Generalmente referido como TN (_True Negative_).\n",
    "* __Verdaderos Positivos__: cantidad de instancias clasificadas como positivas que efectivamente eran positivas. En nuestro ejemplo Benigno es la clase poasitiva porque tiene asociada el uno. Generalmente referido como TP (_True Positive).\n",
    "* __Falsos Negativos__: cantidad de instancias clasificadas como negativas, pero en la realidad era positivas. También llamado Error de Tipo II en estadística. Generalmente referido como FN (_False Negative_).\n",
    "* __Falsos Positivos__: cantidad de instancias clasificadas como positivas, pero en la realidad era negativos. También llamado Error de Tipo I en estadística. Generalmente referido como FP (_False Positive_).\n",
    "\n",
    "\n",
    "__Nota__: Como la regresión logística retorna un valor probabilistico se seleccionó un umbral de 0.5 para discernir entre clasificaciones positivas y negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGif1DZ_Su6J"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "'''Esta función dibuja bonita la matríz de confunsión.\n",
    "'''\n",
    "def show_confusion_matrix(cm, labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Matriz de confusión')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Verdadero')\n",
    "    for i, row in zip(range(len(cm)), cm):\n",
    "        for j, val in zip(range(len(row)), row):\n",
    "            ax.text(i, j, str(val), va='center', ha='center').set_backgroundcolor('white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpdpLS6qSu6V"
   },
   "outputs": [],
   "source": [
    "def logistic_regression(x, w, b):\n",
    "    return 1/(1+tf.exp(-(tf.matmul(x, w) + b)))[:,0]\n",
    "\n",
    "def crossentropy(yt, yp):\n",
    "    return tf.math.reduce_mean(-yt*tf.math.log(tf.clip_by_value(yp, 1e-6, 1)) - (1-yt)*tf.math.log(tf.clip_by_value(1-yp, 1e-6, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xl9J4Z0ZSu6c"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "toC_I2p7Su64"
   },
   "outputs": [],
   "source": [
    "w = tf.random.uniform(shape=[30, 1], minval=-1, maxval=1)\n",
    "b = tf.random.uniform(shape=[], minval=-1, maxval=1)\n",
    "\n",
    "y_pred = logistic_regression(x_test, w, b)\n",
    "show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), labels=['Maligno', 'Benigno'])\n",
    "\n",
    "ciclos = 1000\n",
    "lr = 0.1 \n",
    "errors = []\n",
    "for i in tqdm(range(ciclos)):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([w, b])\n",
    "        loss = crossentropy(y_train, logistic_regression(x_train, w, b))\n",
    "        errors.append(loss.numpy())\n",
    "    gw, gb = g.gradient(loss, [w, b])\n",
    "    w = w - lr * gw\n",
    "    b = b - lr * gb\n",
    "\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))\n",
    "\n",
    "y_pred = logistic_regression(x_test, w, b).numpy()\n",
    "show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), labels=['Maligno', 'Benigno'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQ_gv3igSu7F"
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "Hasta el momento calculamos el gradiente sobre todo el conjunto de datos de entrenamiento. Sin embargo, esto puede no ser posible o eficiente cuando tenemos un conjunto de datos grande. Por esto, se suele dividir el conjunto de datos en mini-batch y entrenar sobre estos mini-batchs. La idea, es que en promedio la agregación de los efectos de la actualización en cada mini-batch nos acerque al mínimo. Obviamente, esto no significa que cada actualización nos acerque al mínimo global.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9RZElIcSu7I"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "w = tf.random.uniform(shape=[30, 1], minval=-1, maxval=1)\n",
    "b = tf.random.uniform(shape=[], minval=-1, maxval=1)\n",
    "ciclos = 100\n",
    "lr = 0.1 \n",
    "errors = []\n",
    "errors_minibatch = []\n",
    "errors_minibatch2 = []\n",
    "for i in range(ciclos):\n",
    "    x_s, y_s = shuffle(x_train, y_train)\n",
    "    for mini_batch in range(0, 500, 50):\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch([w, b])\n",
    "            loss = crossentropy(y_s[mini_batch:mini_batch+50], logistic_regression(x_s[mini_batch:mini_batch+50], w, b))\n",
    "            errors_minibatch2.append(loss.numpy())\n",
    "            errors_minibatch.append(crossentropy(y_train, logistic_regression(x_train, w, b)).numpy())\n",
    "        gw, gb = g.gradient(loss, [w, b])\n",
    "        w = w - lr * gw\n",
    "        b = b - lr * gb\n",
    "    errors.append(crossentropy(y_train, logistic_regression(x_train, w, b)).numpy())\n",
    "\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "plt.plot(errors_minibatch)\n",
    "plt.show()\n",
    "plt.plot(errors_minibatch2)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))\n",
    "y_pred = logistic_regression(x_test, w, b).numpy()\n",
    "show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), labels=['Maligno', 'Benigno'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7AGPY2vSu7N"
   },
   "source": [
    "## Métricas\n",
    "La matriz de confusión es buena para visualizar las capacidades de un clasificador, pero dificulta comparar sus capacidades contra otros modelos. Por este motivo, se han desarrollado diversas métricas para evaluar cuan bien funciona un clasificador. Algunas de esta son:\n",
    "* Accuracy: Porcentaje de predicciones correctas $acc=\\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "* Precision: Porcentaje de predicciones positivas correctas $precision=\\frac{TP}{TP+FP}$\n",
    "* Recall: Porcentaje de las instancias pertenecientas a la clase positiva que fueron clasificadas correctamentes $recall=\\frac{TP}{TP+FN}$\n",
    "* F1-measure: Media harmónica entre la Presicion y el Recall. $F1=2\\frac{precision \\times recall}{precision+recall}$\n",
    "* Matthews correlation coefficient: Es la correlación entre las predicciones y los valores reales. 1 indica predicción perfecta, -1 indica total desacuerdo entre lo predicho y lo real, y 0 indica que el clasificador no aporta ninguna información $MCC = \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }$\n",
    "\n",
    "### Ejercicio\n",
    "Implemente las métricas y apliquelas sobre los resultados del clasificador de ejemplo.\n",
    "__Recuerde__: en el futuro podrá utilizar las implementaciones provistas por scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIWdS2c_Su7O"
   },
   "outputs": [],
   "source": [
    "def tp_tn_fn_fp(y_true, y_pred):\n",
    "    y_pred = y_pred > 0.5\n",
    "    tp = np.sum(y_true * y_pred)\n",
    "    tn = np.sum((1-y_true) * (1-y_pred))\n",
    "    fp = np.sum((1-y_true) * y_pred)\n",
    "    fn = np.sum(y_true * (1-y_pred))\n",
    "    return tp, tn, fn, fp\n",
    "\n",
    "print(tp_tn_fn_fp(y_test, y_pred))\n",
    "\n",
    "#Defina las metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njSPwfOeSu7c"
   },
   "source": [
    "## Problema de OCR de dígitos\n",
    "Para este trabajos utilizaremos el conjunto de datos conocido como [MNIST](http://yann.lecun.com/exdb/mnist/). Este conjunto de datos ya se encuentra dividido entre entrenamiento y testing. El problema consiste en clasificar imagenes de dígistos escritos a mano al dígito correspondiente.\n",
    "\n",
    "| Propiedad | Valor |\n",
    "| --- | --- |\n",
    "| Clases | 10 |\n",
    "| Tamaño de las imagenes | 28 X 28 |\n",
    "| Instancias de entrenemiento | 60.000 |\n",
    "| Instancias de validación | 10.000 |\n",
    "| Valor mínimo de cada pixel | 0 |\n",
    "| Valor máximo de cada pixel | 255 |\n",
    "\n",
    "A continuación se carga el dataset y se dibujan los primeros 100 ejemplos del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAJe_EjRSu8o"
   },
   "source": [
    "## Clasificación multiclase\n",
    "Para la clasificación multiclase una función muy utilizada es el softmax. Esta función toma por entrada un vector y retorna un vectos tal que la suma de todos sus elementos es $1$ y todos los valores están entre $0$ y $1$. Si nuestro problema de clasificación tiene n clases, podemos usar la función softmax y un vector n-dimensiones. Entonces, podemos interpretar la salida de esta función como la distribución de probabilidades de las clases.\n",
    "$$softmax_i(x) = \\frac{e^{x}}{\\sum e^{x}} $$\n",
    "\n",
    "## Categorical Crossentropy\n",
    "Esta función de error considera el error sobre la categoría real, normalizando el valor de la predicción. Considerando $\\hat{y}=(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_C)$ in vector de valores asociados a las clases\n",
    "$$P_\\hat{y}=\\frac{\\hat{y}}{\\sum\\hat{y}_i}$$\n",
    "$$CCE(y,P_\\hat{y})=-\\frac{\\sum y * log(P_\\hat{y})}{N} $$\n",
    "Notese que el valor de error se considera solo sobre las clases verdaderas, las otras son afectadas a través de la normalización de la salida $\\hat{y}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HtLU5JwSu8q"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('100 primeros elementos del conjunto de entrenaimento')\n",
    "f = plt.figure(111)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
    "        ax.set_xticklabels('')\n",
    "        ax.set_yticklabels('')\n",
    "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
    "plt.show()\n",
    "print(y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQLJdJXgSu80"
   },
   "outputs": [],
   "source": [
    "size = x_train.shape[1]*x_train.shape[2]\n",
    "x_train = x_train.reshape((x_train.shape[0], size)) / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], size)) / 255\n",
    "\n",
    "yc_train, yc_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "yc_train = yc_train.astype(np.float32)\n",
    "yc_test = yc_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXoLcAxNSu9H"
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp = tf.exp(z)\n",
    "    return exp / tf.expand_dims(tf.math.reduce_sum(exp, axis=1), axis=-1)\n",
    "\n",
    "def predict(x, w, b):\n",
    "    return softmax(tf.matmul(x, w) + b)\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(tf.clip_by_value(y_pred, 1e-6, 1)), axis=1))\n",
    "\n",
    "def loss_f(y_true, x, w, b):\n",
    "    return categorical_crossentropy(y_true, predict(x, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1UzhbqYSu9T"
   },
   "outputs": [],
   "source": [
    "w = tf.random.uniform(shape=[size, 10], minval=-1, maxval=1)\n",
    "b = tf.random.uniform(shape=[10], minval=-1, maxval=1)\n",
    "ciclos = 100\n",
    "lr = 0.01 \n",
    "batch_size = 500\n",
    "errors = []\n",
    "errors_minibatch = []\n",
    "vw = tf.zeros(shape=[size, 10])\n",
    "vb = tf.zeros(shape=[10])\n",
    "for i in tqdm(range(ciclos)):\n",
    "    x_s, y_s = shuffle(x_train, yc_train)\n",
    "    for mini_batch in range(0, x_s.shape[0], batch_size):\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch([w, b])\n",
    "            loss = loss_f(y_s[mini_batch:mini_batch+batch_size], x_s[mini_batch:mini_batch+batch_size], w, b)\n",
    "            errors_minibatch.append(loss.numpy())\n",
    "        gw, gb = g.gradient(loss, [w, b])\n",
    "        w = w - lr * gw\n",
    "        b = b - lr * gb\n",
    "    errors.append(loss_f(yc_train, x_train, w, b).numpy())\n",
    "    \n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "plt.plot(errors_minibatch)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))\n",
    "y_pred = predict(x_test, w, b)\n",
    "show_confusion_matrix(confusion_matrix(y_test, np.argmax(y_pred , axis=1)), labels=[str(i) for i in range(10)])\n",
    "print(classification_report(y_test, np.argmax(y_pred , axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxdCnYIqWUbC"
   },
   "source": [
    "## Momentum\n",
    "Para mejorar los resultados una técnica muy utilizada es agregar un momentum. La idea del momentum es que vaya llevando una historia del movimiento entre los mini-batchs, y como en promedio esperamos que nos lleve a un mínimo global el momentum llevaría nuestras actualizaciones más hacia el este mínimo.\n",
    "\n",
    "$$vel_n=momentum * vel_{n-1} – lr * grad_n$$\n",
    "\n",
    "$$w_{n+1} = w_{n} + vel_n$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2R-tzEkSu9h"
   },
   "outputs": [],
   "source": [
    "w = tf.random.uniform(shape=[size, 10], minval=-1, maxval=1)\n",
    "b = tf.random.uniform(shape=[10], minval=-1, maxval=1)\n",
    "ciclos = 100\n",
    "lr = 0.01 \n",
    "batch_size = 500\n",
    "momentum = 0.9\n",
    "errors = []\n",
    "errors_minibatch = []\n",
    "vw = tf.zeros(shape=[size, 10])\n",
    "vb = tf.zeros(shape=[10])\n",
    "for i in tqdm(range(ciclos)):\n",
    "    x_s, y_s = shuffle(x_train, yc_train)\n",
    "    for mini_batch in range(0, x_s.shape[0], batch_size):\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch([w, b])\n",
    "            loss = loss_f(y_s[mini_batch:mini_batch+batch_size], x_s[mini_batch:mini_batch+batch_size], w, b)\n",
    "            errors_minibatch.append(loss.numpy())\n",
    "        gw, gb = g.gradient(loss, [w, b])\n",
    "        vw = momentum * vw - lr * gw\n",
    "        vb = momentum * vb - lr * gb\n",
    "        w = w + vw\n",
    "        b = b + vb\n",
    "    errors.append(loss_f(yc_train, x_train, w, b).numpy())\n",
    "    \n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "plt.plot(errors_minibatch)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))\n",
    "y_pred = predict(x_test, w, b)\n",
    "show_confusion_matrix(confusion_matrix(y_test, np.argmax(y_pred , axis=1)), labels=[str(i) for i in range(10)])\n",
    "print(classification_report(y_test, np.argmax(y_pred , axis=1)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03-RegresionLogistica_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
