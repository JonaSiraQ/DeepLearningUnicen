{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8P6jPbVjey3"
   },
   "source": [
    "# Word Embeddings\n",
    "Word Embeddings es una técnica que consiste en asignarle a cada palabra un vector de características que representa el concepto asociado a la palabra. En el siguiente ejemplo, utilizaremos el modelo preentrenado utilizando la técnica Word2Vec[1] utilizando artículos de [Google News](https://code.google.com/archive/p/word2vec/). \n",
    "\n",
    "[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2eCAn6Ojey4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import gensim\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7H7nuTFlhtu"
   },
   "outputs": [],
   "source": [
    "gdown.download(\"https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\", \"GoogleNews-vectors-negative300.bin.gz\", quiet=False)\n",
    "!gunzip -d GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fM2UwFkwlclc"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyUdZJLajezN"
   },
   "outputs": [],
   "source": [
    "words = ['king', 'queen', 'man', 'woman']\n",
    "print('Vector king: {}'.format(model.wv['king']))\n",
    "\n",
    "vec = np.empty((4, 300))\n",
    "for i, w in enumerate(words):\n",
    "    vec[i, :] = model.wv[w]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "x = TSNE().fit_transform(vec)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x[:, 0], x[:, 1])\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x[i, 0], x[i, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBIOq-bijez4"
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fVQkSsYmbG3"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0ImbtCnje0S"
   },
   "source": [
    "## Entrenando word embeddings\n",
    "Una de las técnicas utilizadas para entrenar estos word embeddings se suele definir un clasificador que intenta calcular la probabilidad de la palabra w dado un contexto C, es decir, un conjunto de palabras cercanas. Es decir, nuestro clasificador intenta:\n",
    "\n",
    "$$h(w,C) \\approx P(w|C)$$\n",
    "\n",
    "En general esto se hace a través de una red neuronal superficial.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/OpupG.png\" alt=\"Shallow NN\" style=\"width: 400px;\"/>\n",
    "\n",
    "> Fig. 1: Red Neuronal superficial <br>\n",
    "\n",
    "En esta arquitecturas, cada palabra se representa mediante un ID único, donde cada ID a su vez puede ser traducido en un vector de todos ceros, menos un uno en la posición de ID. Supongamos un vocabulario restringido $VOC=\\{P1, P2, P3, P4\\}$, entonces a la parabra P2 se le asigna el ID 2 y el vector $V(P2)=(0,1,0,0)$. Esta forma de codificación, se conoce como hot-one. El embeddings de las palabras es la matríz de pesos $W$ de la primera capa densa, donde cada columna representa a la palabra asociada. Es decir, el embedding de la palabra P2 sería $W[2,:]$. Es importante notar que:\n",
    "\n",
    "$$V(P2) \\cdot W = (0, 1, 0,0) \\cdot \\left[\\begin{array}{c}\n",
    "W[1,1]\\ldots W[1,n]\\\\\n",
    "W[2,1]\\ldots W[2,n]\\\\\n",
    "W[3,1]\\ldots W[3,n]\\\\\n",
    "W[4,1]\\ldots W[4,n]\n",
    "\\end{array}\\right] = w[2,:]$$\n",
    "\n",
    "## Negative Sampling\n",
    "Para el entrenamiento es sencillo conseguir ejemplos positivos a partir del texto. Por ejemplo si consideramos el texto \"La Argentina está organizada como un Estado federal descentralizado, integrado desde 1994 por un Estado nacional y 24 estados autogobernados\", obtenido del artículo sobre [Argentinina en Wikipedia](https://es.wikipedia.org/wiki/Argentina), es facil ver que en un contexto de 4 palabras alrededor de \"estado\" están las palabras \"está\", \"organizado\", \"como\", \"un\", \"federal\", \"descentralizado\", \"integrado\" y \"desde\". Sin embargo, no hay ejemplos de palabras no asociadas con estado, por lo que se suele utilizar la técnica de sampleo negativos, es decir seleccionar palabras aleatores para generar nuestra muestra negativa.\n",
    "Por ejemplo, consideremos:\n",
    "* Un vocabulario con 10 palabras.\n",
    "* Una oración compuesta por las palabras [0, 1, 2, 3, 4, 5, 6].\n",
    "* Un contexto de 2 palabras. Por ejemplo, el contexto de la palabra 3 serían 1, 2, 4, 5.\n",
    "* Un rate de positive samples y negative samples de 1.\n",
    "Podríamos generar nuestra muestra de entrenamiento de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O862TjVSje0Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "sentence = list(range(0, 7))\n",
    "print(sentence)\n",
    "x, y = skipgrams(sentence, 10, window_size=2, negative_samples=1.0, shuffle=False)\n",
    "print('Skipgrams: ')\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6mdEJCSje01"
   },
   "source": [
    "Puede observarse que en algunos casos una instancia aparece como positiva y negativa al mismo tiempo, sin embargo este problema se mitiga a medida de que el vocabulario se hace más grande. \n",
    "Obviamente, para poder entrenar se necesitan más datos, para esto utilizaremos un conjunto de datos de noticias. Es importante notar que, si bien están clasificadas, esto no es de importancia debido a que Word2Vec es un algoritmo de aprendizaje no supervisado. Esto significa que no necesita datos etiquetados para aprender.\n",
    "El siguiente código levanta los datos y los formatea correctamente. Solo considera palabras con 5 o más repeticiones y le aplica stemming para reducir el espacio de entrenamiento. Los skipgrams consirean una ventana de 5 elemento y una proporción de 5 ejemplos negativos por cada ejemplo verdadero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YVvWRrVje05"
   },
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Baja los stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    'rec.autos',\n",
    "    'rec.motorcycles',\n",
    "    'rec.sport.baseball',\n",
    "    'rec.sport.hockey',\n",
    "    'sci.crypt',\n",
    "    'sci.electronics',\n",
    "    'sci.med',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                                     shuffle=True, random_state=0,\n",
    "                                     remove=remove)\n",
    "\n",
    "from collections import Counter, deque\n",
    "\n",
    "def process_corpus(data, words_id=None, min_reps=5):\n",
    "    corpus = []\n",
    "    for text in tqdm(data):\n",
    "        corpus.append(tokenizer_stem_nostop(preprocessor(text)))\n",
    "        \n",
    "    if words_id is None:\n",
    "        #Cuenta palabras en el corpus\n",
    "        words = Counter()\n",
    "\n",
    "        for s in corpus:\n",
    "            for w in s:\n",
    "                words[w] += 1\n",
    "\n",
    "        #Elimina palabras con menos de 5 repeticiones\n",
    "        words_id = {}\n",
    "        id_next = 0\n",
    "        for w, c in words.items():\n",
    "            if c >= min_reps:\n",
    "                words_id[w] = id_next\n",
    "                id_next += 1\n",
    "\n",
    "    id_words = { v:k for k, v in words_id.items()}\n",
    "    corpus_id = [[words_id[w] for w in s if w in words_id] for s in corpus]\n",
    "    return corpus_id, words_id, id_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuxZwsiqje1N"
   },
   "outputs": [],
   "source": [
    "x = deque()\n",
    "y = deque()\n",
    "\n",
    "corpus_id, words_id, id_words = process_corpus(newsgroups.data)\n",
    "\n",
    "#Crea los skipgrams de entrenamiento\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "for s in tqdm(corpus_id):\n",
    "    x1, y1 = skipgrams(s, len(id_words), window_size=3, negative_samples=5)\n",
    "    x.extend(x1)\n",
    "    y.extend(y1)\n",
    "\n",
    "\n",
    "x1, x2 = zip(*x)\n",
    "import numpy as np\n",
    "x1 = np.asarray(x1)\n",
    "x2 = np.asarray(x2)\n",
    "y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "print('Vocabulario: {}'.format(len(id_words)))\n",
    "print('Skipgrams: {}'.format(x1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAZ_6gf0je1m"
   },
   "source": [
    "## Implementando el clasificador\n",
    "Si bien se desea implementar una red densa, esto es ineficiente. Hay que notar que la representación one-hot hace que la multiplicación sea efectivamente acceder una fila de la matriz $W$, en general:\n",
    "\n",
    "$$V(Pi) \\cdot W = (0, \\ldots, 1, \\ldots,0) \\cdot \\left[\\begin{array}{c}\n",
    "W[1,1]\\ldots W[1,n]\\\\\n",
    "W[2,1]\\ldots W[2,n]\\\\\n",
    "\\ddots \\ddots \\ddots\\\\\n",
    "W[m,1]\\ldots W[m,n]\n",
    "\\end{array}\\right] = w[i,:]$$\n",
    "\n",
    "La capa Embeddings de Keras funciona de esta manera, es decir, recibe vector de ids de palabras(enteros) y retorna una matriz donde cada fila representa el embedding de la palabra. Considerando esto, la matriz de embeddings se puede entrenar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYwI6ClJje16"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import dot, Embedding, Input, Activation, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#Función de error basada en log-likelihood\n",
    "def minus_max_likelihood(y_true, y_pred):\n",
    "    max_like = y_true * K.log(1+ K.exp(-y_pred)) + (1 - y_true) * K.log(1+ K.exp(y_pred)) \n",
    "    return max_like\n",
    "\n",
    "context_emb = Embedding(len(id_words), 64, name='Emb_context')\n",
    "target_emb = Embedding(len(id_words), 64, name='Emb_target')\n",
    "\n",
    "context = Input((1,), name='context')\n",
    "emb = context_emb(context)\n",
    "target = Input((1,), name='target')\n",
    "embT = target_emb(target)\n",
    "lam = dot([emb, embT], axes=(-1))\n",
    "lam = Flatten()(lam) \n",
    "#lam = Activation('sigmoid')(lam)\n",
    "\n",
    "model = Model(inputs=[context, target], outputs=lam)\n",
    "model.compile('adam', minus_max_likelihood)\n",
    "#model.compile('adam', 'binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "#Entrenamos poco \n",
    "model.fit([x1, x2], y.astype(np.float32), epochs=1, batch_size=1000)\n",
    "#Obtención de los embeddings\n",
    "vectors = K.get_value(target_emb.embeddings)\n",
    "#Recuperar memoria\n",
    "del context_emb\n",
    "del target_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsclTaXXje2d"
   },
   "outputs": [],
   "source": [
    "print(vectors.shape)\n",
    "print(words_id['car'])\n",
    "print(vectors[words_id['car'], :])\n",
    "print(vectors[words_id['car'], :].shape)\n",
    "\n",
    "np.dot(vectors[words_id['car'], :], vectors[words_id['ford'], :]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oy7Gm2Gsje3j"
   },
   "source": [
    "## Viendo algún resultado\n",
    "Como antes podemos asumir que las palabras similares están cercanas en el espacio de vectores. Esta similitud la podemos medir por similitud del coseno. Podemos ver palabras similares a \"car\", \"ford\", \"law\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZya-u_Bje30"
   },
   "outputs": [],
   "source": [
    "def cos(v1, v2):\n",
    "    return np.dot(v1, v2.T) / (np.dot(v1, v1.T) ** 0.5 * np.sum(v2 * v2, axis=-1) ** 0.5)\n",
    "\n",
    "\n",
    "def nearest(voc, wv, top=11):\n",
    "    dist = cos(wv, voc)\n",
    "    a = range(len(dist))\n",
    "    a = sorted(a, key=lambda x: dist[x], reverse=True)\n",
    "    return a[0:top], [dist[x] for x in a[0:top]]\n",
    "\n",
    "print('Similares a car:')\n",
    "for i, d in zip(*nearest(vectors, vectors[words_id['car'], :])):\n",
    "    print('\\t{} {}'.format(id_words[i], d))\n",
    "\n",
    "print('Similares a ford:')\n",
    "for i, d in zip(*nearest(vectors, vectors[words_id['ford'], :])):\n",
    "    print('\\t{} {}'.format(id_words[i], d))\n",
    "\n",
    "print('Similares a law:')\n",
    "for i, d in zip(*nearest(vectors, vectors[words_id['law'], :])):\n",
    "    print('\\t{} {}'.format(id_words[i], d))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuIxQAOPoR3E"
   },
   "outputs": [],
   "source": [
    "del x\n",
    "del y\n",
    "del x1\n",
    "del x2\n",
    "del vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVXEG2XMje4c"
   },
   "source": [
    "## GloVe\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/) es otro método para entrenar embeddings basado en matriz de coocurrencias. El objetivo de la red en este caso es predecir la cantidad de coocurrencias de dos palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWiDiCUOje4f"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def bigram_count(token_list, window_size, cache):\n",
    "    sentence_size = len(token_list)\n",
    "\n",
    "    for central_index, central_word_id in enumerate(token_list):\n",
    "        for distance in range(1, window_size + 1):\n",
    "            if central_index + distance < sentence_size:\n",
    "                first_id, second_id = sorted([central_word_id, token_list[central_index + distance]])\n",
    "                cache[first_id][second_id] += 1.0 / distance\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_cooccurrences(sequences, cache, window=3):\n",
    "    for seq in tqdm(sequences):\n",
    "        bigram_count(token_list=seq, cache=cache, window_size=window)\n",
    "\n",
    "\n",
    "def process_coocurrence_matrix(sentences, window_size=3):\n",
    "    cache = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "    build_cooccurrences(sentences, cache=cache, window=window_size)\n",
    "    first, second, x_ijs = deque(), deque(), deque()\n",
    "\n",
    "    for first_id in cache.keys():\n",
    "        for second_id in cache[first_id].keys():\n",
    "            x_ij = cache[first_id][second_id]\n",
    "\n",
    "            first.append(first_id)\n",
    "            second.append(second_id)\n",
    "            x_ijs.append(x_ij)\n",
    "\n",
    "            first.append(second_id)\n",
    "            second.append(first_id)\n",
    "            x_ijs.append(x_ij)\n",
    "\n",
    "    return np.array(first), np.array(second), np.array(x_ijs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyiMOlJkje41"
   },
   "outputs": [],
   "source": [
    "x1, x2, y = process_coocurrence_matrix(corpus_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snOAFSjDje5P"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Add\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred, a = 3.0/4.0, X_MAX=100):\n",
    "    \"\"\"\n",
    "    This is GloVe's loss function\n",
    "    :param y_true: The actual values, in our case the 'observed' X_ij co-occurrence values\n",
    "    :param y_pred: The predicted (log-)co-occurrences from the model\n",
    "    :return: The loss associated with this batch\n",
    "    \"\"\"\n",
    "    return K.sum(K.pow(K.clip(y_true / X_MAX, 0.0, 1.0), a) * K.square(y_pred - K.log(y_true)), axis=-1)\n",
    "\n",
    "\n",
    "def glove_model(vocab_size=10, vector_dim=64):\n",
    "    \"\"\"\n",
    "    A Keras implementation of the GloVe architecture\n",
    "    :param vocab_size: The number of distinct words\n",
    "    :param vector_dim: The vector dimension of each word\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_target = Input((1,), name='central_word_id')\n",
    "    input_context = Input((1,), name='context_word_id')\n",
    "\n",
    "    central_embedding = Embedding(vocab_size+1, vector_dim, input_length=1, name='central_emb')\n",
    "    central_bias = Embedding(vocab_size+1, 1, input_length=1, name='central_bias')\n",
    "\n",
    "    context_embedding = Embedding(vocab_size, vector_dim, input_length=1, name='context_emb')\n",
    "    context_bias = Embedding(vocab_size, 1, input_length=1, name='context_bias')\n",
    "\n",
    "    vector_target = central_embedding(input_target)\n",
    "    vector_context = context_embedding(input_context)\n",
    "\n",
    "    bias_target = central_bias(input_target)\n",
    "    bias_context = context_bias(input_context)\n",
    "\n",
    "    dot_product = Dot(axes=-1)([vector_target, vector_context])\n",
    "    dot_product = Reshape((1, ))(dot_product)\n",
    "    bias_target = Reshape((1,))(bias_target)\n",
    "    bias_context = Reshape((1,))(bias_context)\n",
    "\n",
    "    prediction = Add()([dot_product, bias_target, bias_context])\n",
    "\n",
    "    model = Model(inputs=[input_target, input_context], outputs=prediction)\n",
    "    model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3YvZ9cxje53"
   },
   "outputs": [],
   "source": [
    "model = glove_model(len(words_id), 64)\n",
    "model.summary()\n",
    "\n",
    "model.fit([x1, x2], y, epochs=5, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jyb_03Ihje6V"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "from IPython.display import Image\n",
    "Image(retina=True, filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8LVu-Fkje6p"
   },
   "outputs": [],
   "source": [
    "#Obtención de los embeddings\n",
    "vectors = K.get_value(model.layers[2].embeddings)\n",
    "#Recuperar memoria\n",
    "#del model\n",
    "\n",
    "print('Similares a car:')\n",
    "for i, d in zip(*nearest(vectors, vectors[words_id['car'], :])):\n",
    "    print('\\t{} {}'.format(id_words[i], d))\n",
    "\n",
    "print('Similares a ford:')\n",
    "for i, d in zip(*nearest(vectors, vectors[words_id['ford'], :])):\n",
    "    print('\\t{} {}'.format(id_words[i], d))\n",
    "\n",
    "print('Similares a law:')\n",
    "for i, d in zip(*nearest(vectors, vectors[words_id['law'], :])):\n",
    "    print('\\t{} {}'.format(id_words[i], d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5qZCQEyqiYM"
   },
   "outputs": [],
   "source": [
    "del x1\n",
    "del x2\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5gSTYcVje7F"
   },
   "source": [
    "# Redes Neuronales Recurrentes\n",
    "Tradicionalmente, una de las formas de representar texto para machine learning es utilizar \"bag-of-words\", o alguna variación que cuente la frecuencia de las palabras en el texto y corpus, como [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), ver documentación de [sk-learn](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Supongamos que queremos entrenar un clasificador de opiniones y tenemos la frase \"Pedro no es tonto, es inteligente\" categorizada como positiva. La frase podría representarse en un vector donde cada posición representa una palabra del vocabulario y su valor es la fercuencia en la frase. Entonces, nuestra frase sería representada por un vector ralo: {'Pedro': 1, 'no':1, 'es': 2, 'tonto': 1, 'inteligente': 1}. Pero la frase \"Pedro no es inteligente, es tonto\" tendría la misma representación pero sentido completamente opuesto. Este es un ejemplo donde casos donde las características de las instancias tiene relación de orden, es decir, no basta con conocer las características de la instancia para representarla correctamente sino el orden.\n",
    "\n",
    "Exiten redes neuronales que consideran esta información y se conocen como redes neuronales recurrentes. En estas redes, el valor de salida depende de los valores de entrada procesados de forma secuencial.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg\" alt=\"Shallow NN\" style=\"width: 400px;\"/>\n",
    "\n",
    "> Fig. 2: Red Neuronal Recurrente. Imagen: [Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network) <br>\n",
    "\n",
    "Como se ve, en estas redes, una instancia es una sequencia de elementos. En el caso de texto, la secuencia puede ser una secuencia de vectores embedding generados con alguna técnica como Word2Vec, o entrenados directamente en la red.\n",
    "\n",
    "Una de las capas recurrentes más comuenmente usada es la LSTM. \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/0*LyfY3Mow9eCYlj7o.\" alt=\"Shallow NN\" style=\"width: 400px;\"/>\n",
    "\n",
    "> Fig. 2: LSTM. Imagen: [Codeburst](https://codeburst.io/generating-text-using-an-lstm-network-no-libraries-2dff88a3968) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ha78NWr-je7H"
   },
   "outputs": [],
   "source": [
    "#Cargar datos en formato texto\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "#Cargamos bibliotecas y demás\n",
    "!pip install tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os.path\n",
    "while not os.path.exists('imdb.pkl.gzip'):\n",
    "    #Si no está el archivo hay que subirlo. Solo para Google Colab!!\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = pickle.load(gzip.open('imdb.pkl.gzip', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WUC_nluje7c"
   },
   "outputs": [],
   "source": [
    "print(x_train[0:3])\n",
    "print(y_train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7ro1P4nje76"
   },
   "outputs": [],
   "source": [
    "#Reformateamos usando la estrategia definida arriba.\n",
    "x_train, words_id, _ = process_corpus(x_train)\n",
    "x_test, _, _ = process_corpus(x_test, words_id)\n",
    "idx = words_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRSM-TcVje8f"
   },
   "outputs": [],
   "source": [
    "print(x_train[0:3])\n",
    "print(y_train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnIfLbEqje8v"
   },
   "outputs": [],
   "source": [
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KU9f7Seeje9E"
   },
   "outputs": [],
   "source": [
    "#Creamos una red neuronal recurrente con embedddings\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Input, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "i = Input((None,))\n",
    "d = Embedding(len(idx) + 1, 150, mask_zero=True)(i)\n",
    "d = LSTM(150, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(d)\n",
    "d = LSTM(150, return_sequences=False)(d)\n",
    "d = Dense(100, activation='relu')(d)\n",
    "d = Dense(1, activation='sigmoid')(d)\n",
    "\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGCCMRkAje9h"
   },
   "outputs": [],
   "source": [
    "h = model.fit(x_train, y_train, epochs=2, batch_size=256, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "al5qu_Gkje9l",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Testeamos la red neuronal\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "x_train = pad_sequences(x_train, 50)\n",
    "x_test = pad_sequences(x_test, 50)\n",
    "\n",
    "h = model.fit(x_train, y_train, epochs=5, batch_size=256, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWbJU6x0je9-"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GAaxl289je-f"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('Loss')\n",
    "plt.plot(h.history['loss'], 'r-')\n",
    "plt.plot(h.history['val_loss'], 'b-')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy')\n",
    "plt.plot(h.history['binary_accuracy'], 'r-')\n",
    "plt.plot(h.history['val_binary_accuracy'], 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvwMpeYAje-2"
   },
   "source": [
    "## Auto-Nietzsche\n",
    "En esta sección utilizaremos una red neuronal recurrente para autogenerar texto. Para esto, utilizaremos el ejemplo propuesto por [Keras](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgSggOe0je-4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "path = get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print(sentences[0])\n",
    "print(next_chars[0])\n",
    "print(x[0,:,:])\n",
    "print(y[0,:])\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "on_epoch_end(-1,None)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffw7cYx8je_Q"
   },
   "source": [
    "## Ejercicio\n",
    "Implemente una red neuronal para resolver [Facebook bAbI task](https://research.fb.com/downloads/babi/), para el caso una frase, una pregunta; y dos frases, una pregunta.\n",
    "Pasos a seguir:\n",
    "\n",
    "1. Formatear los datos para la entrada de la red neuronal.\n",
    "1. Definir la arquitectura.\n",
    "1. Entrenar y testear.\n",
    "\n",
    "Se recomienda usar una entrada diferente para cada componente de la entrada. Por ejemplo, para el caso una frase, una pregunta:\n",
    "\n",
    "```\n",
    "x_sentence = ...#Oraciones\n",
    "x_question = ...#Pregunta asociada a la oracion\n",
    "y = ... #Respuesta\n",
    "\n",
    "i_sentence = Input(...)\n",
    "d_sentence = Capa_X()(i_sentence)\n",
    "...\n",
    "d_sentence = Capa_X()(d_sentence)\n",
    "\n",
    "i_question = Input(...)\n",
    "d_question = Capa_X()(i_question)\n",
    "...\n",
    "d_question = Capa_X()(d_question)\n",
    "\n",
    "d = concatenate(d_sentence, d_question) #Hay otras opciones como add, multiply, dot...\n",
    "d = ...\n",
    "d = CapaFinal()(d)\n",
    "\n",
    "model = Model(inputs=[i_sentence, i_question], outputs=d)\n",
    "...\n",
    "\n",
    "model.fit([x_sentence, x_question], y, ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-keVl7Qje_Y"
   },
   "source": [
    "# Attention\n",
    "Este tipo de capas es la base de los transformers que son actualmente utilizados en arquitecturas como BERT o GTP-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xW4K3Pxjje_Z"
   },
   "outputs": [],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FRy7mBEje_q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "def load_imdb_ds(path, shuffle_ds=True, random_state=None):\n",
    "    def load_text(path_sel):\n",
    "        xa = []\n",
    "        for f in os.listdir(path_sel):\n",
    "            f = open(path_sel + os.sep + f, 'r')\n",
    "            xa.append(next(f))\n",
    "        return xa\n",
    "    x = load_text(path + os.sep + 'pos')\n",
    "    y = [1] * len(x)\n",
    "    xn = load_text(path + os.sep + 'neg')\n",
    "    x.extend(xn)\n",
    "    y.extend([0] * len(xn))\n",
    "    if shuffle_ds:\n",
    "        shuffle(x, y, random_state=random_state)\n",
    "    return x, y\n",
    "\n",
    "x_train_text, y_train = load_imdb_ds('aclImdb/train', random_state=42)\n",
    "x_test_text, y_test = load_imdb_ds('aclImdb/test', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3ceYa3yje_5"
   },
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install tqdm\n",
    "\n",
    "from collections import Counter, deque\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Baja los stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "\n",
    "def tokenizer_simple(text):\n",
    "    return [w for w in re.split('\\s+', text.strip()) \\\n",
    "            if re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "\n",
    "def process_corpus(data, words_id=None, min_reps=5, tokenizer=tokenizer_simple):\n",
    "    corpus = []\n",
    "    for text in tqdm(data):\n",
    "        corpus.append(tokenizer(preprocessor(text)))\n",
    "        \n",
    "    if words_id is None:\n",
    "        #Cuenta palabras en el corpus\n",
    "        words = Counter()\n",
    "\n",
    "        for s in corpus:\n",
    "            for w in s:\n",
    "                words[w] += 1\n",
    "\n",
    "        #Elimina palabras con menos de 5 repeticiones\n",
    "        words_id = {}\n",
    "        id_next = 1\n",
    "        for w, c in words.items():\n",
    "            if c >= min_reps:\n",
    "                words_id[w] = id_next\n",
    "                id_next += 1\n",
    "\n",
    "    id_words = { v:k for k, v in words_id.items()}\n",
    "    corpus_id = [[words_id[w] for w in s if w in words_id] for s in corpus]\n",
    "    return corpus_id, words_id, id_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7ns-O9pjfA-"
   },
   "outputs": [],
   "source": [
    "x_train, words_id, id_words = process_corpus(x_train_text)\n",
    "x_test, _, _ = process_corpus(x_test_text, words_id=words_id)\n",
    "y_train = np.expand_dims(np.asarray(y_train), axis=-1)\n",
    "y_test = np.expand_dims(np.asarray(y_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBpTTT17uYEx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAXLEN = 50\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=MAXLEN)\n",
    "x_test = pad_sequences(x_test, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Mrfh8v2jfBb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Attention, Add, GlobalAveragePooling1D, Dropout, Lambda, Embedding, Input, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Attention, Add, GlobalAveragePooling1D, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "i = Input((None,))\n",
    "e = Embedding(len(words_id) + 1, 60, mask_zero=True, name='base_emb')(i)\n",
    "\n",
    "dq = Dense(60)(e)\n",
    "dk = Dense(60)(e)\n",
    "\n",
    "att = Attention()([dq, dk])\n",
    "attd = Dropout(0.1)(att)\n",
    "\n",
    "d = GlobalAveragePooling1D()(attd)\n",
    "d = Dropout(0.1)(d)\n",
    "d = Dense(100)(d)\n",
    "d = Dropout(0.1)(d)\n",
    "d = Dense(1, activation='sigmoid')(d)\n",
    "\n",
    "\n",
    "model = Model(i, d)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "of03BAc7jfBw"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "from IPython.display import Image\n",
    "Image(retina=True, filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBJ5ZU9MjfCW"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqAJEznYjfCq"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "from IPython.display import Image\n",
    "Image(retina=True, filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUZkbxONjfDD"
   },
   "outputs": [],
   "source": [
    "x_exam = ['The movie was excelent. It is probably the best movie ever', 'The movie was not good']\n",
    "x_exam_v, _, _ = process_corpus(x_exam, words_id)\n",
    "x_exam_v = pad_sequences(x_exam_v, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zD0dA9URjfDp"
   },
   "outputs": [],
   "source": [
    "print(model.predict(x_exam_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJ9Iu55IjfDw"
   },
   "outputs": [],
   "source": [
    "model_att = Model(i, [dq, dk])\n",
    "\n",
    "vq, vk = model_att(x_exam_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8WUxTOMjfEH"
   },
   "outputs": [],
   "source": [
    "att_sal = tf.nn.softmax(tf.matmul(vq, vk, transpose_b=True))\n",
    "print(att_sal[0,-10:, -10:])\n",
    "print(att_sal[0,-5:, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iC3pOBukjfE0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(att_sal[0,...])\n",
    "plt.show()\n",
    "plt.imshow(att_sal[0, -10:, -10:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0HKeJkfjfFV"
   },
   "outputs": [],
   "source": [
    "plt.imshow(att_sal[1,...])\n",
    "plt.show()\n",
    "plt.imshow(att_sal[1, -5:, -5:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Alr5hSIqjfFo"
   },
   "outputs": [],
   "source": [
    "print(x_test_text[0])\n",
    "print(model.predict(np.expand_dims(x_test[0, :], axis=0)))\n",
    "vq, vk = model_att(np.expand_dims(x_test[0, :], axis=0))\n",
    "att_sal = tf.nn.softmax(tf.matmul(vq, vk, transpose_b=True))\n",
    "plt.imshow(att_sal[0,...])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ffw7cYx8je_Q"
   ],
   "name": "06-Embeddings_LSTM_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
